from google.colab import drive
drive.mount('/content/drive')
!pip install streamlit
import streamlit as st
import pandas as pd
import numpy as np
import joblib
import matplotlib.pyplot as plt
import seaborn as sns
import os
st.set_page_config(
    page_title="Student Academic Stress Classifier",
    layout="wide",
    initial_sidebar_state="expanded")
@st.cache_resource
def load_artifacts():
    try:
        model = joblib.load('academic_stress_model.pkl')
        scaler = joblib.load('scaler.pkl')
        try:
            le = joblib.load('label_encoder.pkl')
        except:
            le = None
        try:
            feature_names = joblib.load('feature_names.pkl')
        except:
            feature_names = None
        return model, scaler, le, feature_names
    except FileNotFoundError:
        st.error("Model files not found. Please make sure you've trained the model first.")
        return None, None, None, None
def main()
    st.title("Student Academic Stress Classification")
    st.write("This app predicts a student's academic stress level based on various factors.")
    st.write("Fill in the information below and click 'Predict' to see the results.")
    model, scaler, le, feature_names = load_artifacts()
    st.sidebar.title("Navigation")
    app_mode = st.sidebar.selectbox("Choose a page", ["Prediction", "Data Analysis", "About"])
    if app_mode == "Prediction":
        prediction_page(model, scaler, le, feature_names)
    elif app_mode == "Data Analysis":
        data_analysis_page()
    elif app_mode == "About":
        about_page()
def prediction_page(model, scaler, le, feature_names):
    if model is None or scaler is None:
        st.error("Please train the model first before using the prediction feature.")
        return
    if feature_names is None:
        st.error("Could not determine the feature names. Please check if feature_names.pkl exists.")
        st.info("Run the feature extraction script first to create feature_names.pkl")
        return
    st.info(f"Model was trained on {len(feature_names)} features")
    input_data = {}
    for feature in feature_names:
        display_name = feature.replace('_', ' ').title()
        if 'timestamp' in feature.lower():
            date_val = st.date_input(f"{display_name}", value=pd.Timestamp.now())
            input_data[feature] = date_val.toordinal() 
        elif 'academic_stage' in feature.lower():
            stages = ["High School", "Undergraduate", "Graduate", "PhD"]
            stage_mapping = {"High School": 1, "Undergraduate": 2, "Graduate": 3, "PhD": 4}
            selected = st.selectbox(f"{display_name}", stages)
            input_data[feature] = stage_mapping[selected]
        elif 'coping_strategy' in feature.lower():
            strategies = ["Exercise", "Meditation", "Talking to friends", "Professional help", "Other"]
            selected = st.multiselect(f"{display_name}", strategies, default=["Exercise"])
            input_data[feature] = len(selected)  
        elif 'bad_habits' in feature.lower() or 'smoking' in feature.lower() or 'drinking' in feature.lower():
            selected = st.selectbox(f"{display_name}", ["No", "Yes"])
            input_data[feature] = 1 if selected == "Yes" else 0
        elif any(term in feature.lower() for term in ['pressure', 'stress', 'environment', 'quality', 'rate', 'competition']):
            input_data[feature] = st.slider(f"{display_name} (1-5)", 1, 5, 3)
        else:
            input_data[feature] = st.slider(f"{display_name} (1-10)", 1, 10, 5)
    if st.button("Predict Stress Level"):
        input_df = pd.DataFrame([input_data])[feature_names]
        input_df = input_df.apply(pd.to_numeric, errors='coerce')
        if input_df.isnull().any().any():
            st.error("Error: Some input values could not be converted to numbers. Please check your inputs.")
            return
        input_scaled = scaler.transform(input_df)
        prediction = model.predict(input_scaled)
        st.write("Raw prediction value:", prediction[0])
        if le is not None:
            prediction_label = le.inverse_transform(prediction)[0]
            st.success("Predicted Stress Level: **{}**".format(prediction_label))
        else:
            stress_mapping = 
             {0: "No Stress",
              1: "Very Low Stress",
              2: "Low Stress",
              3: "Medium Stress",
              4: "High Stress"}
            if prediction[0] in stress_mapping:
                prediction_label = stress_mapping[prediction[0]]
            else:
                prediction_label = "Unknown (Value: {})".format(prediction[0])

            st.success("Predicted Stress Level: **{}**".format(prediction_label))
        try:
            probabilities = model.predict_proba(input_scaled)[0]
            st.subheader("Prediction Probabilities")
            class_labels = ["No Stress", "Very Low Stress", "Low Stress", "Medium Stress", "High Stress"]
            fig, ax = plt.subplots(figsize=(10, 6))
            colors = ['lightgreen', 'lightblue', 'lightyellow', 'lightcoral', 'salmon']
            bars = ax.bar(class_labels, probabilities, color=colors[:len(probabilities)])
            ax.set_ylabel("Probability")
            ax.set_title("Stress Level Prediction Probabilities")
            plt.xticks(rotation=45, ha='right')
            for bar, prob in zip(bars, probabilities):
                height = bar.get_height()
                ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                        f'{prob:.2%}', ha='center', va='bottom')
            st.pyplot(fig)
            for i, prob in enumerate(probabilities):
                st.write("**{}**: {:.2f}%".format(class_labels[i], prob * 100))
        except Exception as e:
            st.info("Probability information is not available for this model.")
            st.write("Error:", e)
def data_analysis_page():
    st.header("Data Analysis")
    uploaded_file = st.file_uploader("Choose a CSV file", type="csv")
    if uploaded_file is not None:
        try:
            df = pd.read_csv(uploaded_file)
            df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_').str.replace('-', '_')
            st.subheader("Dataset Overview")
            st.dataframe(df.head())
            st.subheader("Dataset Statistics")
            st.write(df.describe())
            target_candidates = ['stress_level', 'stress', 'level', 'target']
            target_col = None
            for col in target_candidates:
                if col in df.columns:
                    target_col = col
                    break
            if target_col is None:
                target_col = df.columns[-1]
            st.subheader("Target Variable Distribution")
            fig, ax = plt.subplots()
            df[target_col].value_counts().plot(kind='bar', ax=ax)
            ax.set_title('Distribution of Stress Levels')
            ax.set_xlabel('Stress Level')
            ax.set_ylabel('Count')
            st.pyplot(fig)
            st.subheader("Feature Names")
            feature_names = df.drop(columns=[target_col]).columns.tolist()
            st.write(f"The dataset contains these {len(feature_names)} features:")
            for i, feature in enumerate(feature_names):
                st.write(f"{i+1}. {feature}")
        except Exception as e:
            st.error(f"Error loading dataset: {e}")
    else:
        st.info("Please upload a CSV file to view data analysis.")
def about_page():
    st.header("About This App")
    st.write("This application uses a Machine Learning model to predict student academic stress levels based on various factors such as academic performance, sleep quality, and social support.")
    st.subheader("How it works:")
    st.write("1. The model was trained on a dataset of student academic stress factors")
    st.write("2. Random Forest algorithm was used for classification")
    st.write("3. The model was optimized using hyperparameter tuning")
    st.subheader("Disclaimer")
    st.info("This tool is for educational purposes only and should not be used as a substitute for professional mental health advice.")
if __name__ == "__main__":
    main()
import kagglehub
path = kagglehub.dataset_download("poushal02/student-academic-stress-real-world-dataset")
print("Path to dataset files:", path)
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.model_selection import GridSearchCV
import joblib
import os
import warnings
warnings.filterwarnings('ignore')
plt.style.use('ggplot')
dataset_path = path
print("Files in the dataset directory:")
for file in os.listdir(dataset_path):
    print(file)
csv_files = [f for f in os.listdir(dataset_path) if f.endswith('.csv')]
print(f"\nCSV files found: {csv_files}")
dataset_file = os.path.join(dataset_path, csv_files[0])
df = pd.read_csv(dataset_file)
print("Dataset Shape:", df.shape)
print("\nFirst 5 rows:")
df.head()
df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_').str.replace('-', '_')
print("Cleaned column names:")
print(df.columns.tolist())
print("Missing values in each column:")
print(df.isnull().sum())
print("\nData types:")
print(df.dtypes)
arget_candidates = ['stress_level', 'stress', 'level', 'target']
target_col = None
for col in target_candidates:
    if col in df.columns:
        target_col = col
        break
if target_col is None:
    target_col = df.columns[-1]
print(f"Using '{target_col}' as target variable")
plt.figure(figsize=(8, 6))
df[target_col].value_counts().plot(kind='bar', color=['skyblue', 'lightcoral', 'lightgreen'])
plt.title('Distribution of Stress Levels')
plt.xlabel('Stress Level')
plt.ylabel('Count')
plt.xticks(rotation=0)
plt.show()
categorical_cols = df.select_dtypes(include=['object']).columns
print("Categorical columns:", categorical_cols.tolist())
if len(categorical_cols) > 0:
    le = LabelEncoder()
    for col in categorical_cols:
        if col != target_col: 
            df[col] = le.fit_transform(df[col])
if df[target_col].dtype == 'object':
    le_target = LabelEncoder()
    df[target_col] = le_target.fit_transform(df[target_col])
    print("Target variable encoded")
    print("Encoded values mapping:")
    for i, class_name in enumerate(le_target.classes_):
        print(f"{i}: {class_name}")
X = df.drop(target_col, axis=1)
y = df[target_col]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
print("Training set shape:", X_train_scaled.shape)
print("Testing set shape:", X_test_scaled.shape)
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.model_selection import GridSearchCV 
import matplotlib.pyplot as plt
import seaborn as sns
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]}
grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid,
                           cv=5, n_jobs=-1, verbose=1, scoring='accuracy')
grid_search.fit(X_train_scaled, y_train)
best_rf_model = grid_search.best_estimator_
y_pred_tuned = best_rf_model.predict(X_test_scaled)
accuracy = accuracy_score(y_test, y_pred_tuned)
print(f"Tuned Model Accuracy: {accuracy:.4f}")
print("\nClassification Report for Tuned Model:")
print(classification_report(y_test, y_pred_tuned))
plt.figure(figsize=(8, 6))
cm_tuned = confusion_matrix(y_test, y_pred_tuned)
sns.heatmap(cm_tuned, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix (Tuned Model)')
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.show()
importances = rf_model.feature_importances_
feature_names = X.columns
feature_importances = pd.DataFrame({
    'feature': feature_names,
    'importance': importances
}).sort_values('importance', ascending=False)
plt.figure(figsize=(12, 8))
sns.barplot(x='importance', y='feature', data=feature_importances)
plt.title('Feature Importances')
plt.tight_layout()
plt.show()
print("Top features:")
print(feature_importances.head(10))
inal_model = grid_search.best_estimator_
y_pred_tuned = final_model.predict(X_test_scaled)
print("Final Model Classification Report:")
print(classification_report(y_test, y_pred_tuned))
joblib.dump(final_model, 'academic_stress_model.pkl')
joblib.dump(scaler, 'scaler.pkl')
if 'le_target' in globals(): 
    joblib.dump(le_target, 'label_encoder.pkl')
print("Model, scaler, and encoder (if applicable) saved successfully!")
feature_names = X.columns.tolist()
joblib.dump(feature_names, 'feature_names.pkl')
print(f"Saved {len(feature_names)} feature names to 'feature_names.pkl'")
print("Feature names:", feature_names)
%%bash
pip freeze > requirements.txt
echo "requirements.txt created with installed Python packages."
import os
if not os.path.exists('requirements.txt'):
    !pip freeze > requirements.txt
    print("requirements.txt created with installed Python packages.")
with open('requirements.txt', 'r') as f:
    requirements = f.read()
print(requirements)
!pip install pyngrok
from pyngrok import ngrok
NGROK_AUTH_TOKEN = "35pNcSvfhCBfLT4PxQhPEuIvyLu_2PqfKri4PRz9UjSoBtzwr" 
ngrok.set_auth_token(NGROK_AUTH_TOKEN)
print("ngrok authtoken set.")
%%writefile app.py
import streamlit as st
import pandas as pd
import numpy as np
import joblib
import matplotlib.pyplot as plt
import seaborn as sns
import os
st.set_page_config(
    page_title="Student Academic Stress Classifier",
    layout="wide",
    initial_sidebar_state="expanded")
@st.cache_resource
def load_artifacts():
    try:
        model = joblib.load('academic_stress_model.pkl')
        scaler = joblib.load('scaler.pkl')
        try:
            le = joblib.load('label_encoder.pkl')
        except:
            le = None
        try:
            feature_names = joblib.load('feature_names.pkl')
        except:
            feature_names = None
        return model, scaler, le, feature_names
    except FileNotFoundError:
        st.error("Model files not found. Please make sure you've trained the model first.")
        return None, None, None, None
def main():
    st.title("Student Academic Stress Classification")
    st.write("This app predicts a student's academic stress level based on various factors.")
    st.write("Fill in the information below and click 'Predict' to see the results.")
    model, scaler, le, feature_names = load_artifacts()
    st.sidebar.title("Navigation")
    app_mode = st.sidebar.selectbox("Choose a page", ["Prediction", "Data Analysis", "About"])
    if app_mode == "Prediction":
        prediction_page(model, scaler, le, feature_names)
    elif app_mode == "Data Analysis":
        data_analysis_page()
    elif app_mode == "About":
        about_page()
def prediction_page(model, scaler, le, feature_names):
    if model is None or scaler is None:
        st.error("Please train the model first before using the prediction feature.")
        return
    if feature_names is None:
        st.error("Could not determine the feature names. Please check if feature_names.pkl exists.")
        st.info("Run the feature extraction script first to create feature_names.pkl")
        return
    st.info(f"Model was trained on {len(feature_names)} features")
    input_data = {}
    for feature in feature_names:
        display_name = feature.replace('_', ' ').title()
        if 'timestamp' in feature.lower():
            date_val = st.date_input(f"{display_name}", value=pd.Timestamp.now())
            input_data[feature] = date_val.toordinal() 
        elif 'academic_stage' in feature.lower():
            stages = ["High School", "Undergraduate", "Graduate", "PhD"]
            stage_mapping = {"High School": 1, "Undergraduate": 2, "Graduate": 3, "PhD": 4}
            selected = st.selectbox(f"{display_name}", stages)
            input_data[feature] = stage_mapping[selected]
        elif 'coping_strategy' in feature.lower():
            strategies = ["Exercise", "Meditation", "Talking to friends", "Professional help", "Other"]
            selected = st.multiselect(f"{display_name}", strategies, default=["Exercise"])
            input_data[feature] = len(selected) 
        elif 'bad_habits' in feature.lower() or 'smoking' in feature.lower() or 'drinking' in feature.lower():
            selected = st.selectbox(f"{display_name}", ["No", "Yes"])
            input_data[feature] = 1 if selected == "Yes" else 0
        elif any(term in feature.lower() for term in ['pressure', 'stress', 'environment', 'quality', 'rate', 'competition']):
            input_data[feature] = st.slider(f"{display_name} (1-5)", 1, 5, 3)
        else:
            input_data[feature] = st.slider(f"{display_name} (1-10)", 1, 10, 5)
    if st.button("Predict Stress Level"):
        input_df = pd.DataFrame([input_data])[feature_names]
        input_df = input_df.apply(pd.to_numeric, errors='coerce')
        if input_df.isnull().any().any():
            st.error("Error: Some input values could not be converted to numbers. Please check your inputs.")
            return
        input_scaled = scaler.transform(input_df)
        prediction = model.predict(input_scaled)
        st.write("Raw prediction value:", prediction[0])
        if le is not None:
            prediction_label = le.inverse_transform(prediction)[0]
            st.success("Predicted Stress Level: **{}**".format(prediction_label))
        else:
            stress_mapping = {
                0: "No Stress",
                1: "Very Low Stress",
                2: "Low Stress",
                3: "Medium Stress",
                4: "High Stress"}
            if prediction[0] in stress_mapping:
                prediction_label = stress_mapping[prediction[0]]
            else:
                prediction_label = "Unknown (Value: {})".format(prediction[0])
            st.success("Predicted Stress Level: **{}**".format(prediction_label))
        try:
            probabilities = model.predict_proba(input_scaled)[0]
            st.subheader("Prediction Probabilities")
            class_labels = ["No Stress", "Very Low Stress", "Low Stress", "Medium Stress", "High Stress"]
            fig, ax = plt.subplots(figsize=(10, 6))
            colors = ['lightgreen', 'lightblue', 'lightyellow', 'lightcoral', 'salmon']
            bars = ax.bar(class_labels, probabilities, color=colors[:len(probabilities)])
            ax.set_ylabel("Probability")
            ax.set_title("Stress Level Prediction Probabilities")
            plt.xticks(rotation=45, ha='right')
            for bar, prob in zip(bars, probabilities):
                height = bar.get_height()
                ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                        f'{prob:.2%}', ha='center', va='bottom')
            st.pyplot(fig)
            for i, prob in enumerate(probabilities):
                st.write("**{}**: {:.2f}% uncommon value (uncomment this for normal app)".format(class_labels[i], prob * 100))
        except Exception as e:
            st.info("Probability information is not available for this model.")
            st.write("Error:", e)
def data_analysis_page():
    st.header("Data Analysis")
    uploaded_file = st.file_uploader("Choose a CSV file", type="csv")
    if uploaded_file is not None:
        try:
            df = pd.read_csv(uploaded_file)
            df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_').str.replace('-', '_')
            st.subheader("Dataset Overview")
            st.dataframe(df.head())
            st.subheader("Dataset Statistics")
            st.write(df.describe())
            target_candidates = ['stress_level', 'stress', 'level', 'target']
            target_col = None
            for col in target_candidates:
                if col in df.columns:
                    target_col = col
                    break
            if target_col is None:
                target_col = df.columns[-1]
            st.subheader("Target Variable Distribution")
            fig, ax = plt.subplots()
            df[target_col].value_counts().plot(kind='bar', ax=ax)
            ax.set_title('Distribution of Stress Levels')
            ax.set_xlabel('Stress Level')
            ax.set_ylabel('Count')
            st.pyplot(fig)
            st.subheader("Feature Names")
            feature_names = df.drop(columns=[target_col]).columns.tolist()
            st.write(f"The dataset contains these {len(feature_names)} features:")
            for i, feature in enumerate(feature_names):
                st.write(f"{i+1}. {feature}")
        except Exception as e:
            st.error(f"Error loading dataset: {e}")
    else:
        st.info("Please upload a CSV file to view data analysis.")
def about_page():
    st.header("About This App")
    st.write("This application uses a Machine Learning model to predict student academic stress levels based on various factors such as academic performance, sleep quality, and social support.")
    st.subheader("How it works:")
    st.write("1. The model was trained on a dataset of student academic stress factors")
    st.write("2. Random Forest algorithm was used for classification")
    st.write("3. The model was optimized using hyperparameter tuning")
    st.subheader("Disclaimer")
    st.info("This tool is for educational purposes only and should not be used as a substitute for professional mental health advice.")
if __name__ == "__main__":
    main()
from pyngrok import ngrok
import subprocess
import os
import time
try:
    !kill $(lsof -t -i:8501)
except subprocess.CalledProcessError:
    pass
except Exception as e:
    print(f"Warning: Could not kill process on port 8501. {e}")
app_path = os.path.join(os.getcwd(), 'app.py')
if not os.path.exists(app_path):
    print(f"Error: app.py not found at {app_path}. Please ensure it was created correctly.")
else:
    print(f"app.py found at {app_path}")
streamlit_command = ['python3', '-m', 'streamlit', 'run', app_path, '--server.port', '8501', '--server.enableCORS', 'false', '--server.enableXsrfProtection', 'false']
streamlit_process = subprocess.Popen(
    streamlit_command,
    stdout=subprocess.PIPE,
    stderr=subprocess.PIPE,
    text=True,
    bufsize=1)
print("Waiting for Streamlit to start...")
time.sleep(45)
if streamlit_process.poll() is not None:
    stdout, stderr = streamlit_process.communicate()
    print("Streamlit process exited unexpectedly during startup.")
    print("Streamlit stdout:", stdout)
    print("Streamlit stderr:", stderr)
    print("Ngrok tunnel will not be started.")
else:
    print("Streamlit process appears to be running. Attempting ngrok connection...")
    try:
        public_url = ngrok.connect(8501)
        print("Streamlit App URL:", public_url)
        print("Please copy and paste this URL into your browser to access the app.")
        print("\n--- Monitoring Streamlit process ---")
        while True:
            if streamlit_process.poll() is not None:
                print("\nStreamlit process has terminated.")
                stdout, stderr = streamlit_process.communicate()
                print("Streamlit stdout:", stdout)
                print("Streamlit stderr:", stderr)
                break
            time.sleep(5) 
    except Exception as e:
        print(f"Error starting ngrok tunnel: {e}")
    finally:
        if streamlit_process.poll() is None:
            print("Streamlit is still running in the background. If you close the notebook, it will stop.")
        else:
            print("Streamlit process has already exited.")
